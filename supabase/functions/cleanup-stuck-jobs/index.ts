import { createClient } from "npm:@supabase/supabase-js@2";

import { corsHeaders } from "../_shared/cors.ts";
import { SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY } from "../_shared/env.ts";

const MAX_RETRIES = 3;
const STUCK_THRESHOLD_MINUTES = 5;

Deno.serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response("ok", { headers: corsHeaders });
  }

  console.log('ğŸ§¹ [Cleanup] Starting stuck jobs cleanup...');
  console.log(`[Cleanup] Timestamp: ${new Date().toISOString()}`);
  
  try {
    const supabase = createClient(SUPABASE_URL!, SUPABASE_SERVICE_ROLE_KEY!);

    const cutoffTime = new Date(Date.now() - STUCK_THRESHOLD_MINUTES * 60 * 1000).toISOString();
    
    console.log(`[Cleanup] Cutoff time: ${cutoffTime}`);
    console.log(`[Cleanup] Looking for jobs stuck in 'running' state...`);
    
    // SÃ©lectionner les jobs bloquÃ©s
    const { data: stuckJobs, error: selectError } = await supabase
      .from('job_queue')
      .select('id, order_id, user_id, created_at, updated_at, retry_count')
      .eq('status', 'running')
      .lt('updated_at', cutoffTime);

    if (selectError) {
      console.error('âŒ [Cleanup] Failed to select stuck jobs:', selectError);
      throw selectError;
    }

    if (!stuckJobs || stuckJobs.length === 0) {
      console.log('âœ… [Cleanup] No stuck jobs found - system healthy');
      return new Response(JSON.stringify({ 
        cleaned: 0,
        retried: 0,
        failed: 0,
        message: 'No stuck jobs found',
        timestamp: new Date().toISOString()
      }), {
        status: 200,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' }
      });
    }

    console.log(`âš ï¸ [Cleanup] Found ${stuckJobs.length} stuck jobs`);

    // SÃ©parer les jobs qui peuvent Ãªtre retentÃ©s de ceux qui doivent Ã©chouer
    const retriableJobs = stuckJobs.filter(j => (j.retry_count || 0) < MAX_RETRIES);
    const failedJobs = stuckJobs.filter(j => (j.retry_count || 0) >= MAX_RETRIES);

    let retriedCount = 0;
    let failedCount = 0;

    // âœ… RETRY : Remettre en queue les jobs qui peuvent Ãªtre retentÃ©s
    // FIX: Update each job individually to correctly increment its own retry_count
    if (retriableJobs.length > 0) {
      console.log(`ğŸ”„ [Cleanup] Retrying ${retriableJobs.length} jobs (retry_count < ${MAX_RETRIES})`);
      
      for (const job of retriableJobs) {
        const newRetryCount = (job.retry_count || 0) + 1;
        const { error: retryError } = await supabase
          .from('job_queue')
          .update({ 
            status: 'queued',
            error: null,
            retry_count: newRetryCount,
            updated_at: new Date().toISOString()
          })
          .eq('id', job.id);

        if (retryError) {
          console.error(`âŒ [Cleanup] Failed to retry job ${job.id.slice(0, 8)}:`, retryError);
        } else {
          retriedCount++;
          console.log(`âœ… [Cleanup] Re-queued job ${job.id.slice(0, 8)} (retry ${newRetryCount}/${MAX_RETRIES})`);
        }
      }
    }

    // âŒ FAIL : Marquer comme failed les jobs qui ont dÃ©passÃ© le max de retries
    if (failedJobs.length > 0) {
      console.log(`ğŸ’€ [Cleanup] Failing ${failedJobs.length} jobs (retry_count >= ${MAX_RETRIES})`);
      
      const { data: markedFailed, error: failError } = await supabase
        .from('job_queue')
        .update({ 
          status: 'failed',
          error: `Timeout - exceeded ${STUCK_THRESHOLD_MINUTES} minutes in running state after ${MAX_RETRIES} retries`,
          updated_at: new Date().toISOString()
        })
        .in('id', failedJobs.map(j => j.id))
        .select('id');

      if (failError) {
        console.error('âŒ [Cleanup] Failed to mark jobs as failed:', failError);
      } else {
        failedCount = markedFailed?.length || 0;
        console.log(`âœ… [Cleanup] Marked ${failedCount} jobs as failed`);
      }
    }

    // Mettre Ã  jour les statuts des orders affectÃ©s par les jobs dÃ©finitivement failed
    const failedOrderIds = [...new Set(failedJobs.map(j => j.order_id).filter(Boolean))];
    console.log(`ğŸ“¦ [Cleanup] Updating ${failedOrderIds.length} affected orders...`);
    
    for (const orderId of failedOrderIds) {
      const { data: allJobs } = await supabase
        .from('job_queue')
        .select('status')
        .eq('order_id', orderId);

      const statuses = allJobs?.map(j => j.status) || [];
      const allDone = statuses.every(s => ['completed', 'failed'].includes(s));
      const anyFailed = statuses.some(s => s === 'failed');

      let orderStatus = 'processing';
      if (allDone) {
        orderStatus = anyFailed ? 'partial' : 'completed';
      }

      await supabase
        .from('orders')
        .update({ status: orderStatus, updated_at: new Date().toISOString() })
        .eq('id', orderId);

      console.log(`âœ… [Cleanup] order ${orderId.slice(0, 8)}... â†’ ${orderStatus}`);
    }

    console.log('ğŸ‰ [Cleanup] Cleanup completed successfully');
    
    return new Response(JSON.stringify({ 
      success: true,
      retried: retriedCount,
      failed: failedCount,
      ordersUpdated: failedOrderIds.length,
      details: {
        retriedJobs: retriableJobs.map(j => j.id.slice(0, 8) + '...'),
        failedJobs: failedJobs.map(j => j.id.slice(0, 8) + '...'),
        timestamp: new Date().toISOString()
      }
    }), {
      status: 200,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });

  } catch (error: any) {
    console.error('âŒ [Cleanup] Critical error:', error);
    console.error('ğŸ“ [Cleanup] Error message:', error.message);
    console.error('ğŸ“ [Cleanup] Error stack:', error.stack);
    return new Response(JSON.stringify({ 
      success: false,
      error: error.message,
      timestamp: new Date().toISOString()
    }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });
  }
});
